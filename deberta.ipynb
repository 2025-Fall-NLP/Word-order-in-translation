{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d354220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "from transformers import DebertaV2Tokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88531a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2Model(\n",
       "  (embeddings): DebertaV2Embeddings(\n",
       "    (word_embeddings): Embedding(251000, 768, padding_idx=0)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): DebertaV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DebertaV2Layer(\n",
       "        (attention): DebertaV2Attention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): DebertaV2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaV2Intermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaV2Output(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 데이터셋 & 모델과 토크나이저 로드\n",
    "# 데이터셋\n",
    "dataset = load_dataset(\"facebook/flores\", \"all\")\n",
    "dev_data = dataset[\"dev\"]  # 997개 문장\n",
    "\n",
    "# 모델 & 토크나이저\n",
    "model_name = \"microsoft/mdeberta-v3-base\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7556dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 사용할 언어 정의\n",
    "langs = {\n",
    "    # 한국어 주변\n",
    "    \"Korean\": \"sentence_kor_Hang\",\n",
    "    \"Japanese\": \"sentence_jpn_Jpan\",\n",
    "    \"Chinese\": \"sentence_zho_Hans\",\n",
    "    \n",
    "    # 영어 주변\n",
    "    \"English\": \"sentence_eng_Latn\",\n",
    "    \"German\": \"sentence_deu_Latn\",\n",
    "    \"French\": \"sentence_fra_Latn\",\n",
    "    \n",
    "    # 기타\n",
    "    \"Spanish\": \"sentence_spa_Latn\",\n",
    "    \"Arabic\": \"sentence_arb_Arab\",\n",
    "}\n",
    "\n",
    "lang_names = list(langs.keys())\n",
    "lang_codes = list(langs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a68a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pooling 함수 정의\n",
    "'''\n",
    "토큰화 ~ 문장의 길이에 비례하는 토큰 수만큼의 벡터가 나옴\n",
    "우리가 필요한 건 문장 하나의 벡터이므로 pooling이 필요하다. \n",
    "'''\n",
    "\n",
    "# [CLS] 토큰(첫 번째 위치)의 출력 사용\n",
    "def cls_pooling(token_embeddings):\n",
    "    return token_embeddings[:, 0, :]\n",
    "\n",
    "# 마스크된 평균 풀링\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * mask_expanded, dim=1)\n",
    "    sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# 마스크된 max pooling\n",
    "def max_pooling(token_embeddings, attention_mask):\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    token_embeddings_masked = token_embeddings.masked_fill(mask_expanded == 0, -1e9)\n",
    "    return torch.max(token_embeddings_masked, dim=1).values\n",
    "\n",
    "def l2_normalize(x):\n",
    "    return x / x.norm(p=2, dim=1, keepdim=True)\n",
    "\n",
    "def cosine_matrix(embeddings):\n",
    "    return torch.matmul(embeddings, embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcacf645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 997 sentences from FLORES-200 dev set...\n",
      "Languages: ['Korean', 'Japanese', 'Chinese', 'English', 'German', 'French', 'Spanish', 'Arabic']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. 각 pooling 방식별 유사도 누적\n",
    "pooling_methods = {\n",
    "    \"CLS\": cls_pooling,\n",
    "    \"Mean\": mean_pooling,\n",
    "    \"Max\": max_pooling\n",
    "}\n",
    "\n",
    "# 결과 저장용 딕셔너리 (pooling 방식별로 유사도 행렬 누적)\n",
    "similarity_sums = {method: torch.zeros(len(langs), len(langs)) for method in pooling_methods.keys()}\n",
    "\n",
    "print(f\"Processing {len(dev_data)} sentences from FLORES-200 dev set...\")\n",
    "print(f\"Languages: {lang_names}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad56312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/997 sentences...\n",
      "Processed 200/997 sentences...\n",
      "Processed 300/997 sentences...\n",
      "Processed 400/997 sentences...\n",
      "Processed 500/997 sentences...\n",
      "Processed 600/997 sentences...\n",
      "Processed 700/997 sentences...\n",
      "Processed 800/997 sentences...\n",
      "Processed 900/997 sentences...\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS: Average Cosine Similarity across 997 sentences\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. 각 문장 처리\n",
    "for idx in range(len(dev_data)):\n",
    "    # 각 언어별 문장 추출\n",
    "    sentences = [dev_data[idx][code] for code in lang_codes]\n",
    "    \n",
    "    # 토큰화\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    \n",
    "    # 모델 추론\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    # 각 pooling 방식별로 처리\n",
    "    for method_name, pooling_func in pooling_methods.items():\n",
    "\n",
    "        if method_name == \"CLS\":\n",
    "            embeddings = pooling_func(token_embeddings)\n",
    "        else:\n",
    "            embeddings = pooling_func(token_embeddings, attention_mask)\n",
    "        \n",
    "        # L2 정규화\n",
    "        embeddings = l2_normalize(embeddings)\n",
    "        \n",
    "        # 코사인 유사도 계산 및 누적\n",
    "        similarity_matrix = cosine_matrix(embeddings)\n",
    "        similarity_sums[method_name] += similarity_matrix\n",
    "    \n",
    "    # 진행상황 출력 (100개마다)\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1}/997 sentences...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"FINAL RESULTS: Average Cosine Similarity across {len(dev_data)} sentences\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "246c73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLS Pooling]\n",
      "------------------------------------------------------------\n",
      "Korean ↔ Japanese: 0.9987\n",
      "Korean ↔ Chinese: 0.9979\n",
      "Korean ↔ English: 0.9984\n",
      "Korean ↔ German: 0.9985\n",
      "Korean ↔ French: 0.9986\n",
      "Korean ↔ Spanish: 0.9986\n",
      "Korean ↔ Arabic: 0.9982\n",
      "\n",
      "[Full Similarity Matrix - CLS Pooling]\n",
      "          Kor     Jap     Chi     Eng     Ger     Fre     Spa     Ara\n",
      "  Kor  1.0000  0.9987  0.9979  0.9984  0.9985  0.9986  0.9986  0.9982\n",
      "  Jap  0.9987  1.0000  0.9985  0.9984  0.9982  0.9980  0.9983  0.9979\n",
      "  Chi  0.9979  0.9985  1.0000  0.9980  0.9977  0.9974  0.9977  0.9978\n",
      "  Eng  0.9984  0.9984  0.9980  1.0000  0.9988  0.9987  0.9989  0.9983\n",
      "  Ger  0.9985  0.9982  0.9977  0.9988  1.0000  0.9989  0.9989  0.9983\n",
      "  Fre  0.9986  0.9980  0.9974  0.9987  0.9989  1.0000  0.9991  0.9983\n",
      "  Spa  0.9986  0.9983  0.9977  0.9989  0.9989  0.9991  1.0000  0.9984\n",
      "  Ara  0.9982  0.9979  0.9978  0.9983  0.9983  0.9983  0.9984  1.0000\n",
      "\n",
      "[Mean Pooling]\n",
      "------------------------------------------------------------\n",
      "Korean ↔ Japanese: 0.9153\n",
      "Korean ↔ Chinese: 0.8936\n",
      "Korean ↔ English: 0.9388\n",
      "Korean ↔ German: 0.9308\n",
      "Korean ↔ French: 0.9005\n",
      "Korean ↔ Spanish: 0.9277\n",
      "Korean ↔ Arabic: 0.9413\n",
      "\n",
      "[Full Similarity Matrix - Mean Pooling]\n",
      "          Kor     Jap     Chi     Eng     Ger     Fre     Spa     Ara\n",
      "  Kor  1.0000  0.9153  0.8936  0.9388  0.9308  0.9005  0.9277  0.9413\n",
      "  Jap  0.9153  1.0000  0.9722  0.8941  0.8333  0.7641  0.8206  0.8355\n",
      "  Chi  0.8936  0.9722  1.0000  0.8754  0.8088  0.7339  0.7927  0.8105\n",
      "  Eng  0.9388  0.8941  0.8754  1.0000  0.9523  0.9185  0.9467  0.9292\n",
      "  Ger  0.9308  0.8333  0.8088  0.9523  1.0000  0.9580  0.9653  0.9457\n",
      "  Fre  0.9005  0.7641  0.7339  0.9185  0.9580  1.0000  0.9635  0.9368\n",
      "  Spa  0.9277  0.8206  0.7927  0.9467  0.9653  0.9635  1.0000  0.9501\n",
      "  Ara  0.9413  0.8355  0.8105  0.9292  0.9457  0.9368  0.9501  1.0000\n",
      "\n",
      "[Max Pooling]\n",
      "------------------------------------------------------------\n",
      "Korean ↔ Japanese: 0.9554\n",
      "Korean ↔ Chinese: 0.9502\n",
      "Korean ↔ English: 0.9571\n",
      "Korean ↔ German: 0.9563\n",
      "Korean ↔ French: 0.9546\n",
      "Korean ↔ Spanish: 0.9566\n",
      "Korean ↔ Arabic: 0.9575\n",
      "\n",
      "[Full Similarity Matrix - Max Pooling]\n",
      "          Kor     Jap     Chi     Eng     Ger     Fre     Spa     Ara\n",
      "  Kor  1.0000  0.9554  0.9502  0.9571  0.9563  0.9546  0.9566  0.9575\n",
      "  Jap  0.9554  1.0000  0.9640  0.9464  0.9427  0.9369  0.9408  0.9407\n",
      "  Chi  0.9502  0.9640  1.0000  0.9443  0.9404  0.9336  0.9373  0.9392\n",
      "  Eng  0.9571  0.9464  0.9443  1.0000  0.9663  0.9648  0.9649  0.9563\n",
      "  Ger  0.9563  0.9427  0.9404  0.9663  1.0000  0.9646  0.9634  0.9561\n",
      "  Fre  0.9546  0.9369  0.9336  0.9648  0.9646  1.0000  0.9669  0.9569\n",
      "  Spa  0.9566  0.9408  0.9373  0.9649  0.9634  0.9669  1.0000  0.9581\n",
      "  Ara  0.9575  0.9407  0.9392  0.9563  0.9561  0.9569  0.9581  1.0000\n",
      "\n",
      "============================================================\n",
      "Processing Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. 평균 계산 및 결과 출력\n",
    "for method_name in pooling_methods.keys():\n",
    "    avg_similarity = similarity_sums[method_name] / len(dev_data)\n",
    "    \n",
    "    print(f\"\\n[{method_name} Pooling]\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 첫 번째 언어(Korean) 기준으로 다른 언어들과의 유사도 출력\n",
    "    base_lang = lang_names[0]\n",
    "    for i in range(1, len(lang_names)):\n",
    "        target_lang = lang_names[i]\n",
    "        similarity = avg_similarity[0, i].item()\n",
    "        print(f\"{base_lang} ↔ {target_lang}: {similarity:.4f}\")\n",
    "    \n",
    "    # 전체 유사도 행렬도 출력 (참고용)\n",
    "    print(f\"\\n[Full Similarity Matrix - {method_name} Pooling]\")\n",
    "    print(\"     \", end=\"\")\n",
    "    for name in lang_names:\n",
    "        print(f\"{name[:3]:>8}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i, name in enumerate(lang_names):\n",
    "        print(f\"{name[:3]:>5}\", end=\"\")\n",
    "        for j in range(len(lang_names)):\n",
    "            print(f\"{avg_similarity[i, j].item():>8.4f}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff58a21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean: 평균 46.0 토큰\n",
      "Japanese: 평균 35.2 토큰\n",
      "Chinese: 평균 36.9 토큰\n",
      "English: 평균 36.2 토큰\n",
      "German: 평균 42.7 토큰\n",
      "French: 평균 50.5 토큰\n",
      "Spanish: 평균 47.6 토큰\n",
      "Arabic: 평균 48.3 토큰\n"
     ]
    }
   ],
   "source": [
    "# (+)\n",
    "# 각 언어별 평균 토큰 수 확인\n",
    "for lang_name, lang_code in langs.items():\n",
    "    token_counts = []\n",
    "    for idx in range(100):  # 샘플 100개\n",
    "        sentence = dev_data[idx][lang_code]\n",
    "        tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        token_counts.append(tokens['input_ids'].shape[1])\n",
    "    \n",
    "    print(f\"{lang_name}: 평균 {np.mean(token_counts):.1f} 토큰\")\n",
    "\n",
    "# 토큰 수 차이가 크면 모델이 언어를 불공평하게 처리할 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
