flores.ipynb
는 flores dataset에서 "동일 의미"의 문장 세트를 불러옴

###############################################################

Deberta.ipynb는
flores.py의 동일 의미 문장 세트를 넣어(ongoing)
임베딩 벡터를 생성(모델은 mberta)
cosine similarity를 측정한다.

?? 왜 mberta가 적합한가?

Wikipedia의 104개 언어 텍스트
각 언어를 따로따로 학습하는 게 아니라
모든 언어를 섞어서 한 번에 MLM(Masked Language Model) 학습
중요한 점: "의도되지 않은 정렬" 즉, 명시적 정렬 없음.

✓ 인위적 번역 쌍 학습 없음
✓ 자연 발생 정렬 = 언어의 본질적 유사성 반영
✓ 구조가 비슷한 언어는 자연스럽게 가까워짐
✓ 문화적/역사적 연결(차용어 등)도 반영

어휘 차용이 많거나
학습 데이터 품질이 비슷하거나
실제로 문장 구조가 의외로 유사할 수 있음.

###############################################################
translation_model.ipynb
1. TED2020으로 원본 영어 문장에 대해, 한국어, 일본어, 중국어 번역된 문장 쌍을 묶어 학습 데이터로 만들어. 
일단 지금은 4가지 언어(영어, 한국어, 일본어, 중국어)만 생각하자. 데이터 개수는 10000개로 일단 실험해보자. 
이것보다 적으면, 최대로 많이. 

2. 기존 mBART의 성능 측정: FLORES-200으로 번역을 돌려서, 
한국어, 일본어, 중국어, 영어 쌍(언어가 4개니까 쌍은 6개겠지?)에 대한 번역 성능을 측정해. 
수치로 번역 성능이 나오면 좋겠지? 이 측정 benchmark는 뭐든 좋아. COMET 등... 

3. 이제 1.의 10000개 데이터셋을 각 6개 독립 모델에 fine-tuning해. 

4. fine-tuning한 6개의 모델의 성능을 다시 측정하여, 성능이 얼마나 늘어났는지를 보기.