{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524cf850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\a\\anaconda3\\envs\\NLP\\lib\\site-packages\\torchmetrics\\utilities\\imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "# requirements\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from comet import download_model, load_from_checkpoint\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff09c077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.23 | packaged by conda-forge | (main, Jun  4 2025, 17:49:16) [MSC v.1929 64 bit (AMD64)]\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 체크\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258ec35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "MBART_MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "# mBART 언어코드\n",
    "MBART_LANGS = {\n",
    "    \"en\": \"en_XX\",\n",
    "    \"ko\": \"ko_KR\",\n",
    "    \"ja\": \"ja_XX\",\n",
    "    \"zh\": \"zh_CN\",\n",
    "}\n",
    "\n",
    "# FLORES 언어코드\n",
    "FLORES_LANGS = {\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"ko\": \"kor_Hang\",\n",
    "    \"ja\": \"jpn_Jpan\",\n",
    "    \"zh\": \"zho_Hans\",\n",
    "}\n",
    "\n",
    "# 실험할 언어쌍 (source -> target)\n",
    "LANGUAGE_PAIRS = [\n",
    "    (\"en\", \"ko\"),\n",
    "    (\"en\", \"ja\"),\n",
    "    (\"en\", \"zh\"),\n",
    "    (\"ko\", \"ja\"),\n",
    "    (\"ko\", \"zh\"),\n",
    "    (\"ja\", \"zh\"),\n",
    "]\n",
    "\n",
    "MAX_TED_SAMPLES = 10000\n",
    "VALID_FRACTION = 0.1  # train : val \n",
    "MAX_FLORES_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debed155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint 관련 함수\n",
    "\n",
    "# 결과 저장\n",
    "def save_checkpoint(filename, data):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Saved checkpoint to {filename}\")\n",
    "\n",
    "# 결과 불러오기\n",
    "def load_checkpoint(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2d985ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ted_multilingual_dataframe(max_samples: int = MAX_TED_SAMPLES):\n",
    "    \n",
    "    print(\"Loading TED2020 from alternative source...\")\n",
    "    \n",
    "    try:\n",
    "        pairs_data = {\n",
    "            'en-ko': load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\", split=\"train\"),\n",
    "            'en-ja': load_dataset(\"Helsinki-NLP/opus-100\", \"en-ja\", split=\"train\"),\n",
    "            'en-zh': load_dataset(\"Helsinki-NLP/opus-100\", \"en-zh\", split=\"train\"),\n",
    "        }\n",
    "        \n",
    "        # 영어 문장을 키로 사용하여 매칭\n",
    "        from collections import defaultdict\n",
    "        en_sentences = defaultdict(dict)\n",
    "        \n",
    "        # en-ko\n",
    "        for ex in pairs_data['en-ko']:\n",
    "            en_text = ex['translation']['en']\n",
    "            ko_text = ex['translation']['ko']\n",
    "            en_sentences[en_text]['en'] = en_text\n",
    "            en_sentences[en_text]['ko'] = ko_text\n",
    "        \n",
    "        # en-ja\n",
    "        for ex in pairs_data['en-ja']:\n",
    "            en_text = ex['translation']['en']\n",
    "            ja_text = ex['translation']['ja']\n",
    "            if en_text in en_sentences:\n",
    "                en_sentences[en_text]['ja'] = ja_text\n",
    "        \n",
    "        # en-zh\n",
    "        for ex in pairs_data['en-zh']:\n",
    "            en_text = ex['translation']['en']\n",
    "            zh_text = ex['translation']['zh']\n",
    "            if en_text in en_sentences:\n",
    "                en_sentences[en_text]['zh'] = zh_text\n",
    "        \n",
    "        # 4개 언어 모두 있는 문장만 선택\n",
    "        rows = []\n",
    "        for sent_dict in en_sentences.values():\n",
    "            if all(lang in sent_dict for lang in ['en', 'ko', 'ja', 'zh']):\n",
    "                rows.append(sent_dict)\n",
    "        \n",
    "        print(f\"Found {len(rows)} multilingual rows with en+ko+ja+zh.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading from opus-100: {e}\")\n",
    "        print(\"Falling back to manual dataset creation...\")\n",
    "        rows = create_synthetic_data()  \n",
    "    \n",
    "    random.shuffle(rows)\n",
    "    if len(rows) > max_samples:\n",
    "        rows = rows[:max_samples]\n",
    "    \n",
    "    ted_dataset = Dataset.from_list(rows)\n",
    "    return ted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d98db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import List, Dict\n",
    "\n",
    "# FLORES 언어 코드 매핑이 필요합니다 (사용자 코드에 이미 정의되어 있다고 가정)\n",
    "# 예시: FLORES_LANGS = {\"en\": \"eng_Latn\", \"ko\": \"kor_Hang\", ...}\n",
    "\n",
    "def load_flores_pairs(src_lang: str, tgt_lang: str, split: str = \"devtest\", max_samples: int = None) -> List[Dict[str, str]]:\n",
    "    \n",
    "    src_flores_code = FLORES_LANGS.get(src_lang)\n",
    "    tgt_flores_code = FLORES_LANGS.get(tgt_lang)\n",
    "\n",
    "    print(f\"Loading FLORES-200 (Parquet) for {src_flores_code} and {tgt_flores_code}...\")\n",
    "\n",
    "    try:\n",
    "        # 변경점: 'facebook/flores' -> 'Muennighoff/flores200'\n",
    "        # 이 리포지토리는 스크립트 없이 Parquet 파일로 제공되므로 trust_remote_code가 필요 없고 에러가 나지 않습니다.\n",
    "        \n",
    "        # 1. 소스 언어 로드\n",
    "        src_dataset = load_dataset(\n",
    "            \"Muennighoff/flores200\", \n",
    "            src_flores_code, \n",
    "            split=split\n",
    "        )\n",
    "        \n",
    "        # 2. 타겟 언어 로드\n",
    "        tgt_dataset = load_dataset(\n",
    "            \"Muennighoff/flores200\", \n",
    "            tgt_flores_code, \n",
    "            split=split\n",
    "        )\n",
    "\n",
    "        pairs = []\n",
    "        # FLORES 데이터셋은 라인 단위 정렬(Line-aligned)이 보장되어 있습니다.\n",
    "        for src_ex, tgt_ex in zip(src_dataset, tgt_dataset):\n",
    "            pairs.append({\n",
    "                \"src\": src_ex['sentence'], \n",
    "                \"ref\": tgt_ex['sentence']\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Primary loading failed: {e}\")\n",
    "        print(\"Trying alternative: opus100 dataset...\")\n",
    "        \n",
    "        # --- 기존 대안 코드 (OPUS-100) ---\n",
    "        try:\n",
    "            pair_name = f\"{src_lang}-{tgt_lang}\"\n",
    "            dataset = load_dataset(\"Helsinki-NLP/opus-100\", pair_name, split=\"test\")\n",
    "            \n",
    "            pairs = []\n",
    "            for ex in dataset:\n",
    "                pairs.append({\n",
    "                    \"src\": ex['translation'][src_lang],\n",
    "                    \"ref\": ex['translation'][tgt_lang]\n",
    "                })\n",
    "        \n",
    "        except Exception as e2:\n",
    "            print(f\"Alternative also failed: {e2}\")\n",
    "            # 최후의 수단: 더미 데이터\n",
    "            pairs = [\n",
    "                {\"src\": \"Hello world\", \"ref\": \"안녕하세요\"},\n",
    "                {\"src\": \"Thank you\", \"ref\": \"감사합니다\"},\n",
    "            ]\n",
    "\n",
    "    if max_samples is not None and len(pairs) > max_samples:\n",
    "        pairs = pairs[:max_samples]\n",
    "\n",
    "    print(f\"Loaded {len(pairs)} pairs for {src_lang}->{tgt_lang} ({split}).\")\n",
    "    return pairs\n",
    "          \n",
    "  \n",
    "\n",
    "\n",
    "def load_mbart():\n",
    "\n",
    "    print(\"Loading mBART model & tokenizer...\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(MBART_MODEL_NAME)  \n",
    "    model = MBartForConditionalGeneration.from_pretrained(MBART_MODEL_NAME)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def mbart_translate_batch(model, tokenizer, texts: List[str], src_lang: str, tgt_lang: str, max_length: int = 256):\n",
    "    mbart_src = MBART_LANGS[src_lang]\n",
    "    mbart_tgt = MBART_LANGS[tgt_lang]\n",
    "\n",
    "    tokenizer.src_lang = mbart_src\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(device)\n",
    "\n",
    "    gen_tokens = model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[mbart_tgt],\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "    )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate_translation_model(model, tokenizer, pairs: List[Dict[str, str]], src_lang: str, tgt_lang: str, batch_size: int = 16):\n",
    "    \"\"\"\n",
    "    BLEU + COMET 평가\n",
    "    \"\"\"\n",
    "    bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "    \n",
    "    all_src = []\n",
    "    all_ref = []\n",
    "    all_hyp = []\n",
    "\n",
    "    for i in tqdm(range(0, len(pairs), batch_size), desc=f\"Translating {src_lang}->{tgt_lang}\"):\n",
    "        batch = pairs[i:i + batch_size]\n",
    "        src_batch = [b[\"src\"] for b in batch]\n",
    "        ref_batch = [b[\"ref\"] for b in batch]\n",
    "\n",
    "        hyp_batch = mbart_translate_batch(model, tokenizer, src_batch, src_lang, tgt_lang)\n",
    "\n",
    "        all_src.extend(src_batch)\n",
    "        all_ref.extend(ref_batch)\n",
    "        all_hyp.extend(hyp_batch)\n",
    "\n",
    "    bleu = bleu_metric.compute(predictions=all_hyp, references=[[r] for r in all_ref])\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        model_path = download_model(\"Unbabel/wmt20-comet-da\")\n",
    "        comet_model = load_from_checkpoint(model_path)\n",
    "        \n",
    "        comet_output = comet_model.predict(\n",
    "            [{\"src\": s, \"mt\": h, \"ref\": r} for s, h, r in zip(all_src, all_hyp, all_ref)],\n",
    "            batch_size=batch_size,\n",
    "            gpus=1 if torch.cuda.is_available() else 0,\n",
    "            num_workers=0,  # 0으로 수정 필요 (windows) - 이후 수정\n",
    "            progress_bar=True,\n",
    "        )\n",
    "        \n",
    "        comet_mean = comet_output.system_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"COMET evaluation failed: {e}\")\n",
    "        print(\"Continuing with BLEU only...\")\n",
    "        comet_mean = None\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu[\"score\"],\n",
    "        \"comet_mean\": comet_mean,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b15326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TED 데이터로 언어쌍별 fine-tuning\n",
    "\n",
    "@dataclass\n",
    "class PairDatasetConfig:\n",
    "    src_lang: str\n",
    "    tgt_lang: str\n",
    "    train_size: int\n",
    "    val_size: int\n",
    "\n",
    "\n",
    "def make_pair_dataset_from_ted(\n",
    "    ted_dataset: Dataset,\n",
    "    src_lang: str,\n",
    "    tgt_lang: str,\n",
    "    train_size: int,\n",
    "    val_size: int,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "\n",
    "    assert src_lang in [\"en\", \"ko\", \"ja\", \"zh\"]\n",
    "    assert tgt_lang in [\"en\", \"ko\", \"ja\", \"zh\"]\n",
    "\n",
    "    src_col = src_lang\n",
    "    tgt_col = tgt_lang\n",
    "\n",
    "    # 필요한 컬럼만 추출\n",
    "    def map_fn(ex):\n",
    "        return {\"src\": ex[src_col], \"tgt\": ex[tgt_col]}\n",
    "\n",
    "    pair_ds = ted_dataset.map(map_fn, remove_columns=ted_dataset.column_names)\n",
    "\n",
    "    # shuffle & split\n",
    "    pair_ds = pair_ds.shuffle(seed=42)\n",
    "    n = len(pair_ds)\n",
    "    train_n = min(train_size, int(n * (1 - VALID_FRACTION)))\n",
    "    val_n = min(val_size, n - train_n)\n",
    "\n",
    "    train_ds = pair_ds.select(range(train_n))\n",
    "    val_ds = pair_ds.select(range(train_n, train_n + val_n))\n",
    "\n",
    "    print(f\"[{src_lang}->{tgt_lang}] Using {len(train_ds)} train / {len(val_ds)} val examples (from {n} total TED entries).\")\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def preprocess_for_mbart(examples, tokenizer, src_lang: str, tgt_lang: str, max_length: int = 128):\n",
    "    mbart_src = MBART_LANGS[src_lang]\n",
    "    mbart_tgt = MBART_LANGS[tgt_lang]\n",
    "\n",
    "    # source 설정\n",
    "    tokenizer.src_lang = mbart_src\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"src\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    tokenizer.set_tgt_lang_special_tokens(mbart_tgt)\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # 원래 src_lang으로 복원\n",
    "    tokenizer.set_src_lang_special_tokens(mbart_src)\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def finetune_mbart_for_pair(\n",
    "    base_model_name: str,\n",
    "    src_lang: str,\n",
    "    tgt_lang: str,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    output_dir: str,\n",
    "    num_train_epochs: int = 3,\n",
    "    batch_size: int = 8,\n",
    "    lr: float = 3e-5,\n",
    "):\n",
    "    \n",
    "    # 한 언어쌍에 대해 mBART fine-tuning.\n",
    "    \n",
    "    print(f\"\\n=== Fine-tuning mBART for {src_lang}->{tgt_lang} ===\")\n",
    "    model = MBartForConditionalGeneration.from_pretrained(base_model_name).to(device)\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(base_model_name)\n",
    "\n",
    "    # tokenizer 초기 설정\n",
    "    mbart_src = MBART_LANGS[src_lang]\n",
    "    mbart_tgt = MBART_LANGS[tgt_lang]\n",
    "    tokenizer.src_lang = mbart_src\n",
    "    tokenizer.tgt_lang = mbart_tgt  # 초기에 tgt_lang도 설정\n",
    "\n",
    "    preprocess_fn = lambda ex: preprocess_for_mbart(ex, tokenizer, src_lang, tgt_lang)\n",
    "    \n",
    "    print(\"Tokenizing train dataset...\")\n",
    "    tokenized_train = train_ds.map(preprocess_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "    \n",
    "    print(\"Tokenizing validation dataset...\")\n",
    "    tokenized_val = val_ds.map(preprocess_fn, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=128,\n",
    "        generation_num_beams=4,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        dataloader_num_workers=0,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"bleu\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        \n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        \n",
    "        # generated ids -> text\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        # -100 -> pad token id\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        result = bleu_metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "        return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Saving model to {output_dir}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3342e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TED multilingual dataset from ted_multilingual_en_ko_ja_zh...\n",
      "Loading mBART model & tokenizer...\n",
      "<Original Model>\n",
      "Original model\n",
      "Skipping en->ko (Found in checkpoint) | BLEU: 1.90, COMET: -0.1025\n",
      "\n",
      "Original model\n",
      "Skipping en->ja (Found in checkpoint) | BLEU: 4.65, COMET: -0.0951\n",
      "\n",
      "Original model\n",
      "Skipping en->zh (Found in checkpoint) | BLEU: 2.55, COMET: 0.1231\n",
      "\n",
      "Original model\n",
      "Skipping ko->ja (Found in checkpoint) | BLEU: 0.00, COMET: -0.1253\n",
      "\n",
      "Original model\n",
      "Skipping ko->zh (Found in checkpoint) | BLEU: 0.00, COMET: 0.5137\n",
      "\n",
      "Original model\n",
      "Skipping ja->zh (Found in checkpoint) | BLEU: 0.00, COMET: 0.3726\n",
      "\n",
      "===========================================================\n",
      "<Fine-tuned model>\n",
      "Fine-tuned model\n",
      "Skipping en->ko (Found in checkpoint) | BLEU: 4.73, COMET: -0.2251\n",
      "\n",
      "Fine-tuned model\n",
      "Skipping en->ja (Found in checkpoint) | BLEU: 7.11, COMET: -0.1730\n",
      "\n",
      "Fine-tuned model\n",
      "Skipping en->zh (Found in checkpoint) | BLEU: 4.61, COMET: 0.1205\n",
      "\n",
      "Fine-tuned model\n",
      "Skipping ko->ja (Found in checkpoint) | BLEU: 0.00, COMET: 0.4012\n",
      "\n",
      "Fine-tuned model\n",
      "Skipping ko->zh (Found in checkpoint) | BLEU: 0.00, COMET: 0.4130\n",
      "\n",
      "Fine-tuned model\n",
      "Skipping ja->zh (Found in checkpoint) | BLEU: 0.00, COMET: 0.7027\n",
      "\n",
      "\n",
      "================ SUMMARY (BASE vs FINETUNED) ================\n",
      "en->ko:\n",
      "  BASE     BLEU 1.90 | COMET -0.1025\n",
      "  FINETUNE BLEU 4.73 | COMET -0.2251\n",
      "  Δ        BLEU 2.83 | COMET -0.1226\n",
      "\n",
      "en->ja:\n",
      "  BASE     BLEU 4.65 | COMET -0.0951\n",
      "  FINETUNE BLEU 7.11 | COMET -0.1730\n",
      "  Δ        BLEU 2.45 | COMET -0.0779\n",
      "\n",
      "en->zh:\n",
      "  BASE     BLEU 2.55 | COMET 0.1231\n",
      "  FINETUNE BLEU 4.61 | COMET 0.1205\n",
      "  Δ        BLEU 2.06 | COMET -0.0027\n",
      "\n",
      "ko->ja:\n",
      "  BASE     BLEU 0.00 | COMET -0.1253\n",
      "  FINETUNE BLEU 0.00 | COMET 0.4012\n",
      "  Δ        BLEU 0.00 | COMET 0.5264\n",
      "\n",
      "ko->zh:\n",
      "  BASE     BLEU 0.00 | COMET 0.5137\n",
      "  FINETUNE BLEU 0.00 | COMET 0.4130\n",
      "  Δ        BLEU 0.00 | COMET -0.1007\n",
      "\n",
      "ja->zh:\n",
      "  BASE     BLEU 0.00 | COMET 0.3726\n",
      "  FINETUNE BLEU 0.00 | COMET 0.7027\n",
      "  Δ        BLEU 0.00 | COMET 0.3301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4. 전체 파이프라인 실행\n",
    "# ----------------------------\n",
    "\n",
    "def main():\n",
    "    # 1. TED2020에서 멀티병렬(EN-KO-JA-ZH) 추출\n",
    "    ted_path = \"ted_multilingual_en_ko_ja_zh\"\n",
    "    if os.path.exists(ted_path):\n",
    "        print(f\"Loading TED multilingual dataset from {ted_path}...\")\n",
    "        ted_dataset = Dataset.load_from_disk(ted_path)\n",
    "    else:\n",
    "        ted_dataset = build_ted_multilingual_dataframe(MAX_TED_SAMPLES)\n",
    "        ted_dataset.save_to_disk(ted_path)\n",
    "\n",
    "    # STEP 2. 기존 mBART 성능 측정 (FLORES-200)\n",
    "    base_model, base_tokenizer = load_mbart()\n",
    "    base_results = {}\n",
    "\n",
    "    base_pkl_path = \"base_results.pkl\"\n",
    "    base_results = load_checkpoint(base_pkl_path)\n",
    "\n",
    "    print(\"<Original Model>\")\n",
    "    for src, tgt in LANGUAGE_PAIRS:\n",
    "        # 이미 결과가 있다면 pass\n",
    "        if (src, tgt) in base_results:\n",
    "            metrics = base_results[(src, tgt)]\n",
    "            print(\"Original model\")\n",
    "            print(f\"Skipping {src}->{tgt} (Found in checkpoint) | \"\n",
    "              f\"BLEU: {metrics.get('bleu', 0):.2f}, \"\n",
    "              f\"COMET: {metrics.get('comet_mean', 0):.4f}\")\n",
    "            print(\"\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Evaluating BASE mBART on FLORES {src}->{tgt} ===\")\n",
    "        \n",
    "        flores_pairs = load_flores_pairs(src, tgt, split=\"devtest\", max_samples=MAX_FLORES_SAMPLES)\n",
    "        metrics = evaluate_translation_model(base_model, base_tokenizer, flores_pairs, src, tgt)\n",
    "        \n",
    "        base_results[(src, tgt)] = metrics\n",
    "        \n",
    "        # 결과 나올 때마다 저장\n",
    "        save_checkpoint(base_pkl_path, base_results)\n",
    "        \n",
    "        print(f\"BASE {src}->{tgt} | BLEU: {metrics['bleu']:.2f}, COMET: {metrics['comet_mean']:.4f}\")\n",
    "\n",
    "    # STEP 3 & 4. 각 언어쌍별 fine-tune 후, FLORES에서 재평가\n",
    "    finetuned_results = {}\n",
    "\n",
    "    ft_pkl_path = \"finetuned_results.pkl\"\n",
    "    finetuned_results = load_checkpoint(ft_pkl_path)\n",
    "\n",
    "    print(\"===========================================================\")\n",
    "    print(\"<Fine-tuned model>\")\n",
    "    for src, tgt in LANGUAGE_PAIRS:\n",
    "\n",
    "        if (src, tgt) in finetuned_results:\n",
    "            metrics = finetuned_results[(src, tgt)]\n",
    "            print(\"Fine-tuned model\")\n",
    "            print(f\"Skipping {src}->{tgt} (Found in checkpoint) | \"\n",
    "              f\"BLEU: {metrics.get('bleu', 0):.2f}, \"\n",
    "              f\"COMET: {metrics.get('comet_mean', 0):.4f}\")\n",
    "            print(\"\")\n",
    "            continue\n",
    "\n",
    "        pair_name = f\"{src}-{tgt}\"\n",
    "        out_dir = f\"mbart_ft_{pair_name}\"\n",
    "\n",
    "        # 3-a. TED에서 해당 언어쌍 Dataset 생성\n",
    "        train_ds, val_ds = make_pair_dataset_from_ted(\n",
    "            ted_dataset,\n",
    "            src_lang=src,\n",
    "            tgt_lang=tgt,\n",
    "            train_size=MAX_TED_SAMPLES,\n",
    "            val_size=int(MAX_TED_SAMPLES * VALID_FRACTION),\n",
    "        )\n",
    "\n",
    "        # 3-b. fine-tuning\n",
    "        ft_model, ft_tokenizer = finetune_mbart_for_pair(\n",
    "            base_model_name=MBART_MODEL_NAME,\n",
    "            src_lang=src,\n",
    "            tgt_lang=tgt,\n",
    "            train_ds=train_ds,\n",
    "            val_ds=val_ds,\n",
    "            output_dir=out_dir,\n",
    "            num_train_epochs=2,\n",
    "            batch_size=8,\n",
    "            lr=3e-5,\n",
    "        )\n",
    "\n",
    "        # 4. FLORES에서 fine-tuned 모델 성능 재측정\n",
    "        print(f\"\\n=== Evaluating FINETUNED mBART ({pair_name}) on FLORES {src}->{tgt} ===\")\n",
    "\n",
    "        ft_model = ft_model.to(device)\n",
    "\n",
    "        flores_pairs = load_flores_pairs(src, tgt, split=\"devtest\", max_samples=MAX_FLORES_SAMPLES)\n",
    "        metrics = evaluate_translation_model(ft_model, ft_tokenizer, flores_pairs, src, tgt)\n",
    "        finetuned_results[(src, tgt)] = metrics\n",
    "\n",
    "        # 결과 저장\n",
    "        save_checkpoint(ft_pkl_path, finetuned_results)\n",
    "        print(f\"FINETUNED {src}->{tgt} | BLEU: {metrics['bleu']:.2f}, COMET: {metrics['comet_mean']:.4f}\")\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"\\n================ SUMMARY (BASE vs FINETUNED) ================\")\n",
    "    for src, tgt in LANGUAGE_PAIRS:\n",
    "        base_m = base_results[(src, tgt)]\n",
    "        ft_m = finetuned_results[(src, tgt)]\n",
    "        print(f\"{src}->{tgt}:\")\n",
    "        print(f\"  BASE     BLEU {base_m['bleu']:.2f} | COMET {base_m['comet_mean']:.4f}\")\n",
    "        print(f\"  FINETUNE BLEU {ft_m['bleu']:.2f} | COMET {ft_m['comet_mean']:.4f}\")\n",
    "        print(f\"  Δ        BLEU {ft_m['bleu']-base_m['bleu']:.2f} | COMET {ft_m['comet_mean']-base_m['comet_mean']:.4f}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
